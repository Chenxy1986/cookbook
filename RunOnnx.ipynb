{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+QQcd7izRLX1baHCBYeYD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeaneigsi/cookbook/blob/main/RunOnnx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PbeK23G2rVEY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVEzlcVzfSYA",
        "outputId": "91a80106-5ef2-410a-cdda-19425c56d297"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '-'...\n",
            "fatal: repository 'https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/tree/main/cuda/' not found\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/tree/main/cuda -"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install --pre onnxruntime-genai"
      ],
      "metadata": {
        "id": "OS7trs4Qfg50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install --pre onnxruntime-genai-directml"
      ],
      "metadata": {
        "id": "36NHGllbl1hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://github.com/microsoft/onnxruntime-genai/blob/main/examples/python/model-qa.py -o model-qa.py\n"
      ],
      "metadata": {
        "id": "cjWslWJahVXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python model-qa.py -m Phi-3-mini-128k-instruct-onnx/directml/directml-int4-awq-block-128"
      ],
      "metadata": {
        "id": "9XpcSmEpiHvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install hf_transfer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY5yM9pDtwZC",
        "outputId": "4364ad61-fe25-4d68-d5b0-f0236e793f3b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hf_transfer\n",
            "  Downloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf_transfer\n",
            "Successfully installed hf_transfer-0.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install huggingface-hub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZCOXYequfE5",
        "outputId": "1a9b74fe-0296-498a-bb15-dd5560f05bb3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download microsoft/Phi-3-mini-128k-instruct-onnx --local-dir ./models --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "id": "a4etKnzuuhI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install --pre onnxruntime-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw5H5niFxQ3I",
        "outputId": "a65ca871-a2c6-43fb-cdcd-27e41f4ef44c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting onnxruntime-genai\n",
            "  Downloading onnxruntime_genai-0.2.0rc3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnxruntime-genai\n",
            "Successfully installed onnxruntime-genai-0.2.0rc3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/microsoft/onnxruntime-genai/main/examples/python/model-qa.py -o model-qa.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJjs5aBBxzMU",
        "outputId": "65658be4-1bae-4662-8a20-7323bd01ff9f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  4094  100  4094    0     0  17340      0 --:--:-- --:--:-- --:--:-- 17421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model-qa.py -m /content/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4  -k 40 -p 0.95 -t 0.8 -r 1.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuxbqa5xzTgc",
        "outputId": "809372be-8382-4164-ce56-bc43fa26ff50"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: salut, tu peux me dire qui es tu ?\n",
            "\n",
            "Output:  *\n",
            "\n",
            "\n",
            "### response ###\n",
            "\n",
            "La phrase que vous avez fournie est une demande de reconnaissance de l'identité de la personne qui parle. En français, pour demander qui est la personne qui parle, on utilise généralement l'expression \"C'est moi\". Cependant, dans le contexte de votre phrase, il semble que vous cherchez une expression qui correspond à \"who are you?\" en anglais.\n",
            "\n",
            "Dans ce cas, la phrase correcte en français serait :\n",
            "\n",
            "\"C'est vous ?\"\n",
            "\n",
            "Cette phrase est une façon informelle et directe de demander à quelqu'un qui il est. Cependant, si vous souhaitez être plus formel ou si vous voulez spécifier que vous demandez qui est la personne qui parle, vous pourriez dire :\n",
            "\n",
            "\"Qui êtes-vous ?\"\n",
            "\n",
            "Cette phrase est plus formelle et directe que \"C'est vous ?\" et est une traduction plus littérale de \"who are you?\".\n",
            "\n",
            "Si vous voulez utiliser une expression qui est plus couramment utilisée et qui est une traduction directe de \"who are you?\", vous pourriez dire :\n",
            "\n",
            "\"Qui êtes-vous ?\"\n",
            "\n",
            "Cette phrase est très courante en français et est utilisée dans des contextes similaires à \"who are you?\".\n",
            "\n",
            "Pour répondre à cette  --control+c pressed, aborting generation--\n",
            "\n",
            "\n",
            "Input: ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cuda inference with ONNX Test"
      ],
      "metadata": {
        "id": "S22MbDXF4bXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install --pre onnxruntime-genai-cuda --index-url=https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-genai/pypi/simple/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaOcn_n_4lND",
        "outputId": "4d276473-cee6-463e-cfca-db30ddabcccd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Looking in indexes: https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-genai/pypi/simple/\n",
            "Requirement already satisfied: onnxruntime-genai-cuda in /usr/local/lib/python3.10/dist-packages (0.2.0rc3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/microsoft/onnxruntime-genai/main/examples/python/model-qa.py -o model-qa.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg2QJXqR8Z2h",
        "outputId": "471dff6c-14e7-4d4c-ab3c-7b5c369ef03b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  4094  100  4094    0     0  10477      0 --:--:-- --:--:-- --:--:-- 10497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whereis libcudart\n",
        "!libcudart: /usr/lib/x86_64-linux-gnu/libcudart.so /usr/share/man/man7/libcudart.7.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsvKwhwk_uew",
        "outputId": "b1de4ed5-79a9-4efd-d1d4-3b49763b7f1d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "libcudart:\n",
            "/bin/bash: line 1: libcudart:: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model-qa.py -m /content/models/cuda/cuda-int4-rtn-block-32   -k 40 -p 0.95 -t 0.8 -r 1.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07YAgt3H6yos",
        "outputId": "d391a248-7691-40a1-9422-99a93eb64c65"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime_genai/__init__.py\", line 11, in <module>\n",
            "    from onnxruntime_genai.onnxruntime_genai import *\n",
            "ImportError: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/model-qa.py\", line 1, in <module>\n",
            "    ﻿import onnxruntime_genai as og\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/onnxruntime_genai/__init__.py\", line 14, in <module>\n",
            "    from onnxruntime_genai.onnxruntime_genai import *\n",
            "ImportError: libcublasLt.so.11: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    }
  ]
}