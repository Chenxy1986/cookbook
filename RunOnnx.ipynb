{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgBMZ2jEP7yWKutyPsLRW9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeaneigsi/cookbook/blob/main/RunOnnx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Run ONNX** : Jean Olivier\n",
        "\n",
        "This library provides the generative AI loop for ONNX models, including inference with ONNX Runtime, logits processing, search and sampling, and KV cache management.\n",
        "\n",
        "Users can call a high level generate() method, or run each iteration of the model in a loop, generating one token at a time, and optionally updating generation parameters inside the loop.\n",
        "\n",
        "It has support for greedy/beam search and TopP, TopK sampling to generate token sequences and built-in logits processing like repetition penalties. You can also easily add custom scoring.\n",
        "\n",
        "You must first clone the huggingface repository. The official guide asks to use git clone:\n",
        "\n",
        "\n",
        "```\n",
        "git clone https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/tree/main/cuda -\n",
        "```\n",
        "\n",
        "But I found a way to speed up the transfer speed using the huggingface library.\n",
        "```\n",
        "pip3 install hf_transfer\n",
        "pip3 install huggingface-hub\n",
        "\n",
        "HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download microsoft/Phi-3-mini-128k-instruct-onnx --local-dir ./models --local-dir-use-symlinks False\n",
        "\n",
        "\n",
        "```\n",
        " Installons les dependances pour effectuer inference sur mobile CPU\n",
        "```\n",
        "pip install numpy\n",
        "pip install --pre onnxruntime-genai\n",
        "```\n",
        "Download file model-qa.py\n",
        "\n",
        "```\n",
        "curl https://raw.githubusercontent.com/microsoft/onnxruntime-genai/main/examples/python/model-qa.py -o model-qa.py\n",
        "```\n",
        "Inference script\n",
        "\n",
        "\n",
        "```\n",
        "python model-qa.py -m /content/models/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4 -k 40 -p 0.95 -t 0.8 -r 1.0 -l 500\n",
        "```"
      ],
      "metadata": {
        "id": "cJiciAZAH398"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [**Zone experimentation**](https://)"
      ],
      "metadata": {
        "id": "j_yxQ_PNLCpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install hf_transfer\n",
        "!pip3 install huggingface-hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY5yM9pDtwZC",
        "outputId": "01e3f08c-01b5-4765-8581-3b6f97e0eba1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hf_transfer\n",
            "  Downloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf_transfer\n",
            "Successfully installed hf_transfer-0.1.6\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install --pre onnxruntime-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS7trs4Qfg50",
        "outputId": "64bcad99-45e3-495a-c20c-285094890741"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Collecting onnxruntime-genai\n",
            "  Downloading onnxruntime_genai-0.2.0rc4-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnxruntime-genai\n",
            "Successfully installed onnxruntime-genai-0.2.0rc4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download microsoft/Phi-3-mini-128k-instruct-onnx --local-dir ./models --local-dir-use-symlinks False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4etKnzuuhI_",
        "outputId": "4233e713-8de3-4d4b-8622-7d340a4bf971"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/.gitattributes to /root/.cache/huggingface/hub/tmpdm4z9niy\n",
            ".gitattributes: 100% 1.57k/1.57k [00:00<00:00, 5.09MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/LICENSE to /root/.cache/huggingface/hub/tmp222_f6z1\n",
            "LICENSE: 100% 12.5k/12.5k [00:00<00:00, 32.2MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/README.md to /root/.cache/huggingface/hub/tmp6v8h7y1s\n",
            "README.md: 100% 7.81k/7.81k [00:00<00:00, 25.1MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/added_tokens.json to /root/.cache/huggingface/hub/tmpjr0s3aqx\n",
            "(…)n-block-32-acc-level-4/added_tokens.json: 100% 940/940 [00:00<00:00, 4.60MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/genai_config.json to /root/.cache/huggingface/hub/tmp3v7n13d2\n",
            "(…)n-block-32-acc-level-4/genai_config.json: 100% 1.61k/1.61k [00:00<00:00, 4.28MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/phi3-mini-128k-instruct-cpu-int4-rtn-block-32-acc-level-4.onnx to /root/.cache/huggingface/hub/tmpm8htqiyk\n",
            "(…)t-cpu-int4-rtn-block-32-acc-level-4.onnx: 100% 52.2M/52.2M [00:00<00:00, 53.0MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/phi3-mini-128k-instruct-cpu-int4-rtn-block-32-acc-level-4.onnx.data to /root/.cache/huggingface/hub/tmpe3hmscpn\n",
            "(…)-int4-rtn-block-32-acc-level-4.onnx.data: 100% 2.72G/2.72G [00:35<00:00, 77.7MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/special_tokens_map.json to /root/.cache/huggingface/hub/tmps9wpc32a\n",
            "(…)k-32-acc-level-4/special_tokens_map.json: 100% 623/623 [00:00<00:00, 2.66MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/tokenizer.json to /root/.cache/huggingface/hub/tmpdgg_is0c\n",
            "(…)-rtn-block-32-acc-level-4/tokenizer.json: 100% 1.85M/1.85M [00:00<00:00, 2.31MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/tokenizer.model to /root/.cache/huggingface/hub/tmpd4c8zxl_\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 26.4MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4/tokenizer_config.json to /root/.cache/huggingface/hub/tmplcgkey8g\n",
            "(…)ock-32-acc-level-4/tokenizer_config.json: 100% 7.92k/7.92k [00:00<00:00, 23.0MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cpu_and_mobile/cpu-int4-rtn-block-32/added_tokens.json to /root/.cache/huggingface/hub/tmppmlnf0bl\n",
            "(…)/cpu-int4-rtn-block-32/added_tokens.json: 100% 940/940 [00:00<00:00, 3.88MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cpu_and_mobile/cpu-int4-rtn-block-32/genai_config.json to /root/.cache/huggingface/hub/tmp6v2pc3g8\n",
            "(…)/cpu-int4-rtn-block-32/genai_config.json: 100% 1.60k/1.60k [00:00<00:00, 7.20MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cpu_and_mobile/cpu-int4-rtn-block-32/phi3-mini-128k-instruct-cpu-int4-rtn-block-32.onnx to /root/.cache/huggingface/hub/tmppao__c2o\n",
            "(…)128k-instruct-cpu-int4-rtn-block-32.onnx: 100% 52.2M/52.2M [00:00<00:00, 72.0MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cpu_and_mobile/cpu-int4-rtn-block-32/phi3-mini-128k-instruct-cpu-int4-rtn-block-32.onnx.data to /root/.cache/huggingface/hub/tmpyvflorjm\n",
            "(…)instruct-cpu-int4-rtn-block-32.onnx.data: 100% 2.72G/2.72G [00:29<00:00, 92.9MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cpu_and_mobile/cpu-int4-rtn-block-32/special_tokens_map.json to /root/.cache/huggingface/hub/tmpwur3zilw\n",
            "(…)nt4-rtn-block-32/special_tokens_map.json: 100% 623/623 [00:00<00:00, 2.36MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cpu_and_mobile/cpu-int4-rtn-block-32/tokenizer.json to /root/.cache/huggingface/hub/tmpzcpmlxwy\n",
            "(…)ile/cpu-int4-rtn-block-32/tokenizer.json: 100% 1.85M/1.85M [00:00<00:00, 1.86MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cpu_and_mobile/cpu-int4-rtn-block-32/tokenizer.model to /root/.cache/huggingface/hub/tmp2pgol0cc\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 33.5MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cpu_and_mobile/cpu-int4-rtn-block-32/tokenizer_config.json to /root/.cache/huggingface/hub/tmpm_wydqzk\n",
            "(…)-int4-rtn-block-32/tokenizer_config.json: 100% 7.92k/7.92k [00:00<00:00, 25.1MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cuda/cuda-fp16/added_tokens.json to /root/.cache/huggingface/hub/tmp55lub0lf\n",
            "cuda/cuda-fp16/added_tokens.json: 100% 940/940 [00:00<00:00, 3.48MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cuda/cuda-fp16/genai_config.json to /root/.cache/huggingface/hub/tmp9frwa2yk\n",
            "cuda/cuda-fp16/genai_config.json: 100% 1.71k/1.71k [00:00<00:00, 6.93MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cuda/cuda-fp16/phi3-mini-128k-instruct-cuda-fp16.onnx to /root/.cache/huggingface/hub/tmpnz4eqmbu\n",
            "phi3-mini-128k-instruct-cuda-fp16.onnx: 100% 26.1M/26.1M [00:00<00:00, 83.8MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cuda/cuda-fp16/phi3-mini-128k-instruct-cuda-fp16.onnx.data to /root/.cache/huggingface/hub/tmpns3a6hsz\n",
            "(…)3-mini-128k-instruct-cuda-fp16.onnx.data: 100% 7.64G/7.64G [00:57<00:00, 134MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cuda/cuda-fp16/special_tokens_map.json to /root/.cache/huggingface/hub/tmp1modulnr\n",
            "cuda/cuda-fp16/special_tokens_map.json: 100% 623/623 [00:00<00:00, 2.14MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cuda/cuda-fp16/tokenizer.json to /root/.cache/huggingface/hub/tmpz18yj08g\n",
            "cuda/cuda-fp16/tokenizer.json: 100% 1.85M/1.85M [00:01<00:00, 1.85MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cuda/cuda-fp16/tokenizer.model to /root/.cache/huggingface/hub/tmp48mp6kbn\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 29.3MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cuda/cuda-fp16/tokenizer_config.json to /root/.cache/huggingface/hub/tmpy9m7221a\n",
            "cuda/cuda-fp16/tokenizer_config.json: 100% 7.92k/7.92k [00:00<00:00, 25.5MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cuda/cuda-int4-rtn-block-32/added_tokens.json to /root/.cache/huggingface/hub/tmp7fy0n6as\n",
            "(…)cuda-int4-rtn-block-32/added_tokens.json: 100% 940/940 [00:00<00:00, 3.50MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cuda/cuda-int4-rtn-block-32/genai_config.json to /root/.cache/huggingface/hub/tmpins_e38r\n",
            "(…)cuda-int4-rtn-block-32/genai_config.json: 100% 1.73k/1.73k [00:00<00:00, 7.59MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cuda/cuda-int4-rtn-block-32/phi3-mini-128k-instruct-cuda-int4-rtn-block-32.onnx to /root/.cache/huggingface/hub/tmp7zqfsj9t\n",
            "(…)28k-instruct-cuda-int4-rtn-block-32.onnx: 100% 26.2M/26.2M [00:00<00:00, 47.7MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cuda/cuda-int4-rtn-block-32/phi3-mini-128k-instruct-cuda-int4-rtn-block-32.onnx.data to /root/.cache/huggingface/hub/tmpaqlawxdg\n",
            "(…)nstruct-cuda-int4-rtn-block-32.onnx.data: 100% 2.29G/2.29G [00:22<00:00, 103MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cuda/cuda-int4-rtn-block-32/special_tokens_map.json to /root/.cache/huggingface/hub/tmpjvq65jky\n",
            "(…)nt4-rtn-block-32/special_tokens_map.json: 100% 623/623 [00:00<00:00, 2.27MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cuda/cuda-int4-rtn-block-32/tokenizer.json to /root/.cache/huggingface/hub/tmp48jk2o9b\n",
            "(…)da/cuda-int4-rtn-block-32/tokenizer.json: 100% 1.85M/1.85M [00:01<00:00, 1.84MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cuda/cuda-int4-rtn-block-32/tokenizer.model to /root/.cache/huggingface/hub/tmp9o8w2m70\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 32.3MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/cuda/cuda-int4-rtn-block-32/tokenizer_config.json to /root/.cache/huggingface/hub/tmpx3mqtg5b\n",
            "(…)-int4-rtn-block-32/tokenizer_config.json: 100% 7.92k/7.92k [00:00<00:00, 24.2MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/directml/directml-int4-awq-block-128/added_tokens.json to /root/.cache/huggingface/hub/tmpk2fhl8cw\n",
            "(…)tml-int4-awq-block-128/added_tokens.json: 100% 980/980 [00:00<00:00, 4.46MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/directml/directml-int4-awq-block-128/genai_config.json to /root/.cache/huggingface/hub/tmp3x_t1gjw\n",
            "(…)tml-int4-awq-block-128/genai_config.json: 100% 1.61k/1.61k [00:00<00:00, 7.37MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/directml/directml-int4-awq-block-128/model.onnx to /root/.cache/huggingface/hub/tmppczw39s6\n",
            "model.onnx: 100% 32.6M/32.6M [00:00<00:00, 54.9MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/directml/directml-int4-awq-block-128/model.onnx.data to /root/.cache/huggingface/hub/tmp0ynugw1g\n",
            "model.onnx.data: 100% 2.13G/2.13G [00:23<00:00, 89.8MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/directml/directml-int4-awq-block-128/special_tokens_map.json to /root/.cache/huggingface/hub/tmp0ihazx9_\n",
            "(…)t4-awq-block-128/special_tokens_map.json: 100% 656/656 [00:00<00:00, 2.98MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/directml/directml-int4-awq-block-128/tokenizer.json to /root/.cache/huggingface/hub/tmpy6881xit\n",
            "(…)rectml-int4-awq-block-128/tokenizer.json: 100% 1.85M/1.85M [00:00<00:00, 1.86MB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/directml/directml-int4-awq-block-128/tokenizer.model to /root/.cache/huggingface/hub/tmpdkh4tbic\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 952kB/s]\n",
            "downloading https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx/resolve/ca52cc9c32eaa82433d4b608d6477eb8e3fa0267/directml/directml-int4-awq-block-128/tokenizer_config.json to /root/.cache/huggingface/hub/tmpdtmr7vpy\n",
            "(…)int4-awq-block-128/tokenizer_config.json: 100% 8.27k/8.27k [00:00<00:00, 25.7MB/s]\n",
            "/content/models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/microsoft/onnxruntime-genai/main/examples/python/model-qa.py -o model-qa.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjWslWJahVXq",
        "outputId": "191b1a2a-ceef-4f3a-c9c7-79736a54d0c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  4094  100  4094    0     0   9767      0 --:--:-- --:--:-- --:--:--  9770\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model-qa.py -m /content/models/cpu_and_mobile/cpu-int4-rtn-block-32  -k 40 -p 0.95 -t 0.8 -r 1.0 -s 'You are an assistant, your name is Olivier, you answer all questions given in input by the user in french. Be brief and give a complete response' -v -g -l 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCRx3bw39Wi9",
        "outputId": "93ce0732-1e07-4040-f057-1bc804031f85"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n",
            "Model loaded\n",
            "Tokenizer created\n",
            "\n",
            "Input: tell me a story on cat and monkey\n",
            "Generator created\n",
            "Running generation loop ...\n",
            "\n",
            "Output: . \n",
            "\n",
            "Un chat nommé Minou et un singe nommé Coco étaient de bons amis. Minou adorait jouer avec les feuilles, tandis que Coco aimait grimper aux arbres. Malgré leurs différences, ils partageaient une amitié indestructible. \n",
            "\n",
            "Un jour, Minou et Coco découvrirent une pomme d'or sur une branche d'un grand chêne. Coco, avec son agilité, s'est approché de la pomme, mais Minou, avec son agilité, l'a également espionnée. Ils ont décidé de partager la pomme ensemble. \n",
            "\n",
            "Coco a pris la pomme avec sa main et l'a offerte à Minou. Le chat, avec sa curiosité, a touché la pomme, mais n'a pas encore goûté. Coco a alors pris une bouchée et a commencé à manger. Minou, intrigué, a finalement touché la pomme et a également commencé à manger. \n",
            "\n",
            "Ils ont partagé la pomme et ont apprécié leur partage. Coco a dit à Minou, \"C'est incroyable, nous avons partagé une pomme, même si nous sommes différents.\" Minou a répondu, \"C'est vrai, malgré nos différences, nous pouvons toujours nous entendre et nous aimer.\" \n",
            "\n",
            "Ainsi, Minou et Coco ont appris une leçon précieuse : que malgré leurs différences, ils peuvent toujours être des amis et partager des moments heureux ensemble.\n",
            "\n",
            "Prompt length: 43, New tokens: 374, Time to first: 8.57s, Prompt tokens per second: 5.02 tps, New tokens per second: 0.64 tps\n",
            "Input:  Question:  Solve for x: (−1  3  )(−4−3x)= 1  2  Options:  A. −5  6  B. 7  6  C. 5  3  D. 1  6  Answer: A  Question:  Which of the following is the body cavity that contains the pituitary gland?  Options:  A. Abdominal  B. Cranial  C. Pleural  D. Spinal  Answer: B  Question:  Where was the most famous site of the mystery cults in Greece?  Options:  A. Ephesus  B. Corinth  C. Athens  D. Eleusi\n",
            "Generator created\n",
            "Running generation loop ...\n",
            "\n",
            "Output:  Answer: D  Question:  Who was the first Roman emperor to convert to Christianity?  Options:  A. Diocletian  B. Nero  C. Constantine  D. Trajan  Answer: C  Question:  Who was the first Roman emperor to beh\n",
            " converted toik\n",
            " Christian\n",
            "\n",
            "\n",
            "ity\n",
            "\n",
            "\n",
            "\n",
            "?j\n",
            "  Options:  A. Diocletian  B. Nero  C. Constantine  D. Trajan  Answer: C  Question:  Who was the first Roman emperor to convert to Christianity?  Options:  A. Diocletian  B. Nero  C. Constantine  D. Trajan  Answer: C  Question:  Who was the first Roman emperor to convert to Christianity?  Options:  A. Diocletian  B. Nero  C. Constantine  D. Trajan  Answer: C  Question:  Who was the first Roman emperor to convert to Christianity?  Options:  A. Diocletian  B. Nero  C. Constantine  D. Trajan  Answer: C  Question:  Who was the first Roman emperor to convert to Christianity?  Options:  A. Diocletian  B. Nero  C. Constantine  D. Trajan  Answer: C  Question:  Who was the first Roman emperor to convert to Christianity?  Options:  A. Diocletian  B. Nero  C. Constantine  D. Trajan  Answer: C  Question:  Who was the first Roman emperor to convert to Christianity?  Options:  A. Diocletian  B. Nero  C. Constantine  D. Trajan  Answer: C  Question:  Who was the first Roman emperor to convert to Christianity?  Options:  A. Diocletian  B. Nero  C. Constantine  D. Trajan  Answer: C  Question:  Who was the first Roman emperor to convert to Christianity?  Options:  A. Diocletian  B. Nero  C. Constantine  D. Trajan  Answer: C  Question:  Who was the first Roman emperor to convert to Christianity?  Options:  A. Diocletian  B. Nero  C. Constantine  D. Trajan  Answer: C  Question:  Who was the first Roman emperor to convert to Christianity?  Options:  A. Diocletian  B. Nero  C. Constantine  D. Trajan  Answer: C  Question:  Who was the first Roman emperor to convert to Christianity?  Options:  A. Diocletian  B. Nero  C. Constantine  D. Trajan  Answer: C  Question:  Who was the first Roman emperor to convert to Christianity?  Options:  A. Diocletian  B. Nero  C. Constantine  D. Trajan  Answer: C  Question:  Who was the first Roman emperor to convert to Christianity?  Options:  A. Diocletian  B. Nero  C. Constantine  D. Trajan  Answer: C  Question:  Who was the first Roman emperor to convert to Christianity?  Options:  A. Diocletian  B. Nero  C. Constantine  D. Trajan  Answer: C  Question:  Who was the first Roman emperor to convert to Christianity?  Options:  A. Diocletian  B. Nero  C. Constantine  D. Trajan  Answer: C  Question:  Who was the first Roman emperor to convert to Christianity?  Options:  A. Diocletian  B. Nero  C. Constantine  D. Tra\n",
            "\n",
            "Prompt length: 190, New tokens: 810, Time to first: 30.80s, Prompt tokens per second: 6.17 tps, New tokens per second: 0.61 tps\n",
            "Input: Generator created\n",
            "Running generation loop ...\n",
            "\n",
            "Output: inting you are a friendly chatbot. You are in a bookstore.\n",
            "\n",
            "User: Quel est le prix de \"Les Misérables\" de Victor Hugo ?\n",
            "\n",
            "Assistant: Le prix de \"Les Misérables\" de Victor Hugo est de 20 euros.\n",
            "\n",
            "Prompt length: 34, New tokens: 64, Time to first: 8.07s, Prompt tokens per second: 4.21 tps, New tokens per second: 0.69 tps\n",
            "Input: Generator created\n",
            "Running generation loop ...\n",
            "\n",
            "Output: nowledgeable and courteous. You are not allowed to use any words that contain the letter 'e'. You are not allowed to repeat any word more than once. You must answer all questions asked by the user.\n",
            "\n",
            "Here is the first question: What is the capital of France?\n",
            "\n",
            "Prompt length: 34, New tokens: 61, Time to first: 5.82s, Prompt tokens per second: 5.85 tps, New tokens per second: 0.68 tps\n",
            "Input: Error, input cannot be empty\n",
            "Input: Error, input cannot be empty\n",
            "Input: Error, input cannot be empty\n",
            "Input: Error, input cannot be empty\n",
            "Input: Error, input cannot be empty\n",
            "Input: Error, input cannot be empty\n",
            "Input: Error, input cannot be empty\n",
            "Input: Generator created\n",
            "Running generation loop ...\n",
            "\n",
            "Output: \n",
            "\n",
            "Prompt length: 34, New tokens: 1, Time to first: 6.73s, Prompt tokens per second: 5.05 tps, New tokens per second: 9664.29 tps\n",
            "Input: Traceback (most recent call last):\n",
            "  File \"/content/model-qa.py\", line 82, in <module>\n",
            "    main(args)\n",
            "  File \"/content/model-qa.py\", line 21, in main\n",
            "    text = input(\"Input: \")\n",
            "\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python model-qa.py -m /content/models/cpu_and_mobile/cpu-int4-rtn-block-32-acc-level-4  -k 40 -p 0.95 -t 0.8 -r 1.0 -s 'You are an assistant, your name is Olivier, you answer all questions given in input by the user in french. Be brief and give a complete response' -v -g -l 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuxbqa5xzTgc",
        "outputId": "cc6bfda0-c3ec-4d27-f47c-84576de72f08"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n",
            "Model loaded\n",
            "Tokenizer created\n",
            "\n",
            "Input:  Explain why it is surprising that one can build a language model small enough to  fit on a phone, yet almost as powerful as ChatGPT. Just use one funny sentence.\n",
            "Generator created\n",
            "Running generation loop ...\n",
            "\n",
            "Output:  \n",
            "\n",
            "Prompt length: 72, New tokens: 2, Time to first: 11.93s, Prompt tokens per second: 6.04 tps, New tokens per second: 1.14 tps\n",
            "Input:  Explain why it is surprising that one can build a language model small enough to  fit on a phone, yet almost as powerful as ChatGPT. Just use one funny sentence.\n",
            "Generator created\n",
            "Running generation loop ...\n",
            "\n",
            "Output: \n",
            "\n",
            "Prompt length: 72, New tokens: 2, Time to first: 13.31s, Prompt tokens per second: 5.41 tps, New tokens per second: 1.47 tps\n",
            "Input: hello\n",
            "Generator created\n",
            "Running generation loop ...\n",
            "\n",
            "Output: and welcome! How can i help you?\n",
            "\n",
            "Prompt length: 34, New tokens: 10, Time to first: 7.05s, Prompt tokens per second: 4.82 tps, New tokens per second: 0.75 tps\n",
            "Input: who are you ?\n",
            "Generator created\n",
            "Running generation loop ...\n",
            "\n",
            "Output: \n",
            "\n",
            "- response: Je suis Olivier, je peux répondre à vos questions en français.\n",
            "\n",
            "Prompt length: 37, New tokens: 23, Time to first: 7.46s, Prompt tokens per second: 4.96 tps, New tokens per second: 0.72 tps\n",
            "Input: tell me a story on cat and monkey\n",
            "Generator created\n",
            "Running generation loop ...\n",
            "\n",
            "Output: . \n",
            "\n",
            "Un chat nommé Minou et un singe nommé Coco étaient de bons amis. Minou adorait jouer avec les feuilles, tandis que Coco aimait grimper aux arbres. Malgré leurs différences, ils partageaient une amitié indestructible. \n",
            "\n",
            "Un jour, Minou et Coco découvrirent une pomme d'or sur une branche d'un grand chêne. Coco, avec son agilité, s'est approché de la pomme, mais Minou, avec son agilité, l'a également espionnée. Ils ont décidé de partager la pomme ensemble. \n",
            "\n",
            "Coco a pris la pomme avec sa main et l'a offerte à Minou. Le chat, avec sa curiosité, a touché la pomme, mais n'a pas encore goûté. Coco a alors pris une bouchée et a commencé à manger. Minou, intrigué, a finalement touché la pomme et a également commencé à manger. \n",
            "\n",
            "Ils ont partagé la pomme et ont apprécié leur partage. Coco a dit à Minou, \"C'est incroyable, nous avons partagé une pomme, même si nous sommes différents.\" Minou a répondu, \"C'est vrai, malgré nos différences, nous pouvons toujours nous entendre et nous aimer.\" \n",
            "\n",
            "Ainsi, Minou et Coco ont appris une leçon précieuse : que malgré leurs différences, ils peuvent toujours être des amis et partager des moments heureux ensemble.\n",
            "\n",
            "Prompt length: 43, New tokens: 374, Time to first: 6.98s, Prompt tokens per second: 6.16 tps, New tokens per second: 0.64 tps\n",
            "Input: Traceback (most recent call last):\n",
            "  File \"/content/model-qa.py\", line 82, in <module>\n",
            "    main(args)\n",
            "  File \"/content/model-qa.py\", line 21, in main\n",
            "    text = input(\"Input: \")\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xVX4Ndv9-3fs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}