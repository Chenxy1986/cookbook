{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1S3VrM_iqxVJluh18HFxStbvdepkyZYel",
      "authorship_tag": "ABX9TyP7hgH4ySetndLp0Ap07Qm1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeaneigsi/cookbook/blob/main/LlamaCpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://llama-cpp-python.readthedocs.io/en/latest/api-reference/?ref=localhost#llama_cpp.Llama"
      ],
      "metadata": {
        "id": "yLn5nUos3Z-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nouvelle section"
      ],
      "metadata": {
        "id": "J6I2XV_qIuqN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzTxkzaOI5HA",
        "outputId": "d5facac9-5fa0-495c-9f25-1b23f0ad4132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cpu\n",
            "Collecting llama-cpp-python\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.63/llama_cpp_python-0.2.63-cp310-cp310-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.63\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.63)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python \\\n",
        "  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu\n",
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir"
      ],
      "metadata": {
        "id": "A1Xvc29fy5kW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install hf_transfer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JH8K7yIyyEW",
        "outputId": "e5fafa8e-28b4-4347-a744-6877f1cbe78d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hf_transfer\n",
            "  Downloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: hf_transfer\n",
            "Successfully installed hf_transfer-0.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q5_K_M.gguf --local-dir . --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eYlteo4y4g2",
        "outputId": "a6d0cc3e-9e8f-4330-fc9c-14c825d0d958"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf to /root/.cache/huggingface/hub/tmpj417pxf1\n",
            "mistral-7b-instruct-v0.2.Q5_K_M.gguf:  62% 3.18G/5.13G [00:39<00:31, 61.9MB/s]^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install huggingface-hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKHhKAZGa7PN",
        "outputId": "34ee06da-d933-427c-b201-6f646f165c2e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt= \"\"\"\n",
        "### System:\n",
        "You are an AI assistant that follows instructions exceptionally well. Be as helpful as possible.\n",
        "### User:\n",
        "List the top 10 APIs\n",
        "### Assistant:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "iZY-4jsIh-pG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
        "llm = Llama(\n",
        "  model_path=\"/content/mistral-7b-instruct-v0.2.Q5_K_M.gguf\",  # Download the model file first\n",
        "  n_ctx= 10000, # The max sequence length to use - note that longer sequence lengths require much more resource\n",
        "  n_batch=512,\n",
        "  n_gpu_layers=-1\n",
        ")"
      ],
      "metadata": {
        "id": "sGBRHAPYgErF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxeO0ImdjBsa",
        "outputId": "f1601ed7-ea7e-41ba-e7ab-8bf12cac2bd1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "98917451199920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\n",
        "   \"Q: Who is the president of USA? A: \",\n",
        "   max_tokens=20000,\n",
        "   stop=[\"Q:\", \"\\n\"],\n",
        "   echo=True\n",
        ")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKOkpuaVB47C",
        "outputId": "2a22d833-e934-4f0c-d8c2-bb936c3c36d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    7378.75 ms\n",
            "llama_print_timings:      sample time =      31.72 ms /    59 runs   (    0.54 ms per token,  1859.79 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7378.60 ms /    13 tokens (  567.58 ms per token,     1.76 tokens per second)\n",
            "llama_print_timings:        eval time =   45555.18 ms /    58 runs   (  785.43 ms per token,     1.27 tokens per second)\n",
            "llama_print_timings:       total time =   53169.71 ms /    71 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'cmpl-730faab7-2954-4d49-9776-23cdc730f3fc', 'object': 'text_completion', 'created': 1713689420, 'model': '/content/mistral-7b-instruct-v0.2.Q5_K_M.gguf', 'choices': [{'text': 'Q: Who is the president of USA? A:  As of my knowledge up to 2021, the President of the United States is Joe Biden. He assumed office on January 20, 2021. However, please check the most recent and reliable sources for the current information as political situations can change rapidly.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 58, 'total_tokens': 71}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output=output['choices'][0]['text']\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu3x3FSiCG2Q",
        "outputId": "78be14b6-7a59-403c-c092-5f88ed46c9bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Who is the president of USA? A:  As of my knowledge up to 2021, the President of the United States is Joe Biden. He assumed office on January 20, 2021. However, please check the most recent and reliable sources for the current information as political situations can change rapidly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community langchain_core langchain"
      ],
      "metadata": {
        "id": "wi18t0bqWjEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "oB9WVRGZWjlF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "b-Z9XMFdW2oR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
      ],
      "metadata": {
        "id": "xv2Ved2QXB9C"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n",
        "    temperature=0.75,\n",
        "    max_tokens=2000,\n",
        "    top_p=1,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,  # Verbose is required to pass to the callback manager\n",
        ")"
      ],
      "metadata": {
        "id": "JyCmmgpeXKAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "Question: A rap battle between Stephen Colbert and John Oliver\n",
        "\"\"\"\n",
        "llm.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "1eIJrSd9XSXG",
        "outputId": "c0670c69-f0e6-44b6-de42-9eac69b7d658"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: (Verse 1 - Stephen Colbert)\n",
            "Yo, it's Colbert, the king of satire,\n",
            "I've got a mouth full of wit, like a lard bucket,\n",
            "But don't get it twisted, I may be a clown, but I'll knock your crown off.\n",
            "\n",
            "(Verse 2 - John Oliver)\n",
            "Stephen, it's John Oliver, the underdog with the upper hand,\n",
            "You may have a mouth full of words, but I've got a brain full of facts,\n",
            "So while you're busy clowning around, I'll be over here, delivering truth bombs.\n",
            "\n",
            "(Verse 3 - Stephen Colbert)\n",
            "John, you may think you've got the upper hand,\n",
            "But let me remind you, it's not about who can spit the most facts,\n",
            "It's about who can make people laugh and think at the same time,\n",
            "And when it comes to that, I'm still the king of satire.\n",
            "\n",
            "(Verse 4 - John Oliver)\n",
            "Stephen, I may have given you the microphone for this round,\n",
            "But let me make one thing clear, when it comes to rap battles, I may not be the reigning champion,\n",
            "But I guarantee you, if you step into the ring with me, it's gonna be a battle royale of epic proportions.\n",
            "\n",
            "(Outro)\n",
            "So there you have it, folks, a rap battle for the ages between Stephen Colbert and John Oliver, two titans of satire, each with their own unique style and approach to delivering laughs and making people think. And while they may be fierce rivals in the world of comedy, one thing is certain, when it comes to making people laugh and think, Stephen Colbert and John Oliver are two of the greatest masters of satire the world has ever seen.\n",
            "\n",
            "So there you have it, folks, a rap battle for the ages between two legends of satire, Stephen Colbert and John Oliver. And while they may be fierce rivals in the world of comedy, one thing is certain, when it comes to making people laugh and think, Stephen Colbert and John Oliver are two of the greatest masters of satire the world has ever seen.\n",
            "\n",
            "So there"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    4972.11 ms\n",
            "llama_print_timings:      sample time =     287.15 ms /   496 runs   (    0.58 ms per token,  1727.30 tokens per second)\n",
            "llama_print_timings: prompt eval time =    8842.10 ms /    16 tokens (  552.63 ms per token,     1.81 tokens per second)\n",
            "llama_print_timings:        eval time =  384802.85 ms /   495 runs   (  777.38 ms per token,     1.29 tokens per second)\n",
            "llama_print_timings:       total time =  396726.85 ms /   511 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Answer: (Verse 1 - Stephen Colbert)\\nYo, it's Colbert, the king of satire,\\nI've got a mouth full of wit, like a lard bucket,\\nBut don't get it twisted, I may be a clown, but I'll knock your crown off.\\n\\n(Verse 2 - John Oliver)\\nStephen, it's John Oliver, the underdog with the upper hand,\\nYou may have a mouth full of words, but I've got a brain full of facts,\\nSo while you're busy clowning around, I'll be over here, delivering truth bombs.\\n\\n(Verse 3 - Stephen Colbert)\\nJohn, you may think you've got the upper hand,\\nBut let me remind you, it's not about who can spit the most facts,\\nIt's about who can make people laugh and think at the same time,\\nAnd when it comes to that, I'm still the king of satire.\\n\\n(Verse 4 - John Oliver)\\nStephen, I may have given you the microphone for this round,\\nBut let me make one thing clear, when it comes to rap battles, I may not be the reigning champion,\\nBut I guarantee you, if you step into the ring with me, it's gonna be a battle royale of epic proportions.\\n\\n(Outro)\\nSo there you have it, folks, a rap battle for the ages between Stephen Colbert and John Oliver, two titans of satire, each with their own unique style and approach to delivering laughs and making people think. And while they may be fierce rivals in the world of comedy, one thing is certain, when it comes to making people laugh and think, Stephen Colbert and John Oliver are two of the greatest masters of satire the world has ever seen.\\n\\nSo there you have it, folks, a rap battle for the ages between two legends of satire, Stephen Colbert and John Oliver. And while they may be fierce rivals in the world of comedy, one thing is certain, when it comes to making people laugh and think, Stephen Colbert and John Oliver are two of the greatest masters of satire the world has ever seen.\\n\\nSo there\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = prompt | llm"
      ],
      "metadata": {
        "id": "1Kp47CKzYfFX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
        "llm_chain.invoke({\"question\": question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "8CHtomllYK8o",
        "outputId": "f283d6f0-0d07-4ac7-d93e-c15cbed61a12"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "First, we need to find out when Justin Bieber was born. A quick internet search will reveal that he was born on March 1, 1994.\n",
            "\n",
            "Now, let's find out which NFL team won the Super Bowl in the year Justin Bieber was born.\n",
            "\n",
            "The Super Bowl for the year 1994 was held on January 30, 1994. The teams that made it to the Super Bowl were the Buffalo Bills and the Dallas Cowboys.\n",
            "\n",
            "In the end, it was the Dallas Cowboys who emerged victorious in Super Bowl XXVIII against the Buffalo Bills.\n",
            "\n",
            "So, there you have it! The NFL team that won the Super Bowl in the year Justin Bieber was born was the Dallas Cowboys."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    4972.11 ms\n",
            "llama_print_timings:      sample time =      95.70 ms /   172 runs   (    0.56 ms per token,  1797.21 tokens per second)\n",
            "llama_print_timings: prompt eval time =   25850.84 ms /    45 tokens (  574.46 ms per token,     1.74 tokens per second)\n",
            "llama_print_timings:        eval time =  132042.80 ms /   171 runs   (  772.18 ms per token,     1.30 tokens per second)\n",
            "llama_print_timings:       total time =  158930.45 ms /   216 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nFirst, we need to find out when Justin Bieber was born. A quick internet search will reveal that he was born on March 1, 1994.\\n\\nNow, let's find out which NFL team won the Super Bowl in the year Justin Bieber was born.\\n\\nThe Super Bowl for the year 1994 was held on January 30, 1994. The teams that made it to the Super Bowl were the Buffalo Bills and the Dallas Cowboys.\\n\\nIn the end, it was the Dallas Cowboys who emerged victorious in Super Bowl XXVIII against the Buffalo Bills.\\n\\nSo, there you have it! The NFL team that won the Super Bowl in the year Justin Bieber was born was the Dallas Cowboys.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nproc --all #CONNAITRE LE NOMBRE DE THREADS\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5-DSq8EgSuX",
        "outputId": "3fcfa79b-333c-4bba-9a92-9d0f116356d1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# other test"
      ],
      "metadata": {
        "id": "J2j8XlhaCCO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QbR7iE-ADZLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore with PowerInfer\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# https://github.com/SJTU-IPADS/PowerInfer\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "3A_ckX8r8RAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SJTU-IPADS/PowerInfer\n",
        "%cd PowerInfer\n",
        "!pip install -r requirements.txt # install Python helpers' dependencies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtDQIuG5D3bw",
        "outputId": "aae6984a-7297-41d2-83bd-830f99f4a046"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PowerInfer'...\n",
            "remote: Enumerating objects: 8824, done.\u001b[K\n",
            "remote: Counting objects: 100% (607/607), done.\u001b[K\n",
            "remote: Compressing objects: 100% (360/360), done.\u001b[K\n",
            "remote: Total 8824 (delta 328), reused 483 (delta 237), pack-reused 8217\u001b[K\n",
            "Receiving objects: 100% (8824/8824), 12.19 MiB | 16.48 MiB/s, done.\n",
            "Resolving deltas: 100% (6142/6142), done.\n",
            "/content/PowerInfer/PowerInfer/PowerInfer\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 5, in <module>\n",
            "    from pip._internal.cli.main import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 10, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n",
            "    from pip._internal.build_env import get_runnable_pip\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/build_env.py\", line 19, in <module>\n",
            "    from pip._internal.cli.spinners import open_spinner\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n",
            "    from pip._internal.utils.logging import get_indentation\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/logging.py\", line 29, in <module>\n",
            "    from pip._internal.utils.misc import ensure_dir\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/utils/misc.py\", line 43, in <module>\n",
            "    from pip._internal.exceptions import CommandError, ExternallyManagedEnvironment\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/exceptions.py\", line 18, in <module>\n",
            "    from pip._vendor.requests.models import Request, Response\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/requests/__init__.py\", line 43, in <module>\n",
            "    from pip._vendor import urllib3\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/__init__.py\", line 13, in <module>\n",
            "    from .connectionpool import HTTPConnectionPool, HTTPSConnectionPool, connection_from_url\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/urllib3/connectionpool.py\", line 12, in <module>\n",
            "    from .connection import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 674, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 576, in module_from_spec\n",
            "  File \"<frozen importlib._bootstrap>\", line 49, in _new_module\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cmake -S . -B build\n",
        "!cmake --build build --config Release"
      ],
      "metadata": {
        "id": "GGylK0m_Ki49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download --resume-download --local-dir ReluLLaMA-7B --local-dir-use-symlinks False PowerInfer/ReluLLaMA-7B-PowerInfer-GGUF"
      ],
      "metadata": {
        "id": "vRHm4kE1KueD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#./build/bin/main -m /PATH/TO/MODEL -n $output_token_count -t $thread_num -p $prompt"
      ],
      "metadata": {
        "id": "Lg8kzKAWNPbx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./build/bin/main -m /content/PowerInfer/ReluLLaMA-7B/llama-7b-relu.powerinfer.gguf -n 128 -t 2 -p \"Once upon a time\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXx2rzZdLPyB",
        "outputId": "80298f75-a44f-40ac-c847-cda47f59c464"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: ./build/bin/main: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion : **Extremement lent PowerInfer**"
      ],
      "metadata": {
        "id": "-3a-V1wZPIxZ"
      }
    }
  ]
}