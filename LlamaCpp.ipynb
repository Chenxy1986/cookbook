{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1S3VrM_iqxVJluh18HFxStbvdepkyZYel",
      "authorship_tag": "ABX9TyNVkSIlK6RNbN7+4bcsXPKj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeaneigsi/cookbook/blob/main/LlamaCpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzTxkzaOI5HA",
        "outputId": "599e388b-d382-4ac6-8ade-85273c7b2385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cpu\n",
            "Collecting llama-cpp-python\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.63/llama_cpp_python-0.2.63-cp310-cp310-linux_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m987.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.63\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.63)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-cpp-python \\\n",
        "  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu\n",
        "!pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install hf_transfer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JH8K7yIyyEW",
        "outputId": "17049c7c-7836-4de3-b269-c1288c61d49f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.10/dist-packages (0.1.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF mistral-7b-instruct-v0.2.Q5_K_M.gguf --local-dir . --local-dir-use-symlinks False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eYlteo4y4g2",
        "outputId": "f7b8785d-4940-4684-b02e-2b583154d632"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf to /root/.cache/huggingface/hub/tmpo57yloz6\n",
            "mistral-7b-instruct-v0.2.Q5_K_M.gguf: 100% 5.13G/5.13G [00:41<00:00, 123MB/s]\n",
            "./mistral-7b-instruct-v0.2.Q5_K_M.gguf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install huggingface-hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKHhKAZGa7PN",
        "outputId": "282afaa1-0f15-4dca-d9cb-67508f3c4b1e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt= \"\"\"\n",
        "### System:\n",
        "You are an AI assistant that follows instructions exceptionally well. Be as helpful as possible.\n",
        "### User:\n",
        "List the top 10 APIs\n",
        "### Assistant:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "iZY-4jsIh-pG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
        "llm = Llama(\n",
        "  model_path=\"/content/mistral-7b-instruct-v0.2.Q5_K_M.gguf\",  # Download the model file first\n",
        "  n_ctx=20000  # The max sequence length to use - note that longer sequence lengths require much more resource\n",
        ")"
      ],
      "metadata": {
        "id": "sGBRHAPYgErF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\n",
        "   \"Q: Who is the president of USA? A: \",\n",
        "   max_tokens=20000,\n",
        "   stop=[\"Q:\", \"\\n\"],\n",
        "   echo=True\n",
        ")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKOkpuaVB47C",
        "outputId": "bd42d6ac-7b56-4f41-8956-7a88a5156524"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    7311.48 ms\n",
            "llama_print_timings:      sample time =      47.66 ms /    79 runs   (    0.60 ms per token,  1657.61 tokens per second)\n",
            "llama_print_timings: prompt eval time =    7310.90 ms /    13 tokens (  562.38 ms per token,     1.78 tokens per second)\n",
            "llama_print_timings:        eval time =   66837.78 ms /    78 runs   (  856.89 ms per token,     1.17 tokens per second)\n",
            "llama_print_timings:       total time =   74520.83 ms /    91 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'cmpl-fbae687c-1b1c-431e-b0e3-3eb126de46ae', 'object': 'text_completion', 'created': 1713659576, 'model': '/content/mistral-7b-instruct-v0.2.Q5_K_M.gguf', 'choices': [{'text': 'Q: Who is the president of USA? A:  Joe Biden is the President of the United States as of January 20, 2021. He assumed office following his victory in the presidential elections held on November 3, 2020. Prior to Biden, Donald Trump served as the President from January 20, 2017, to January 20, 2021.', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 78, 'total_tokens': 91}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output=output['choices'][0]['text']\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bu3x3FSiCG2Q",
        "outputId": "3777ac7a-4d7c-4af8-8c13-0d0c6267b263"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Who is the president of USA? A:  As of my knowledge up to 2021, the President of the United States is Joe Biden. He assumed office on January 20, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community langchain_core langchain"
      ],
      "metadata": {
        "id": "wi18t0bqWjEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "oB9WVRGZWjlF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "b-Z9XMFdW2oR"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
      ],
      "metadata": {
        "id": "xv2Ved2QXB9C"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"/content/mistral-7b-instruct-v0.2.Q5_K_M.gguf\",\n",
        "    temperature=0.75,\n",
        "    max_tokens=2000,\n",
        "    top_p=1,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,  # Verbose is required to pass to the callback manager\n",
        ")"
      ],
      "metadata": {
        "id": "JyCmmgpeXKAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"\n",
        "Question: A rap battle between Stephen Colbert and John Oliver\n",
        "\"\"\"\n",
        "llm.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "1eIJrSd9XSXG",
        "outputId": "e7db64a1-4d08-4ad5-e8b7-87f4cbdb8217"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: (Verse 1 - Stephen Colbert)\n",
            "Yo, I'm the Colbert Report,\n",
            "I got the wit, I got the report,\n",
            "Now John Oliver, you think you're so slick,\n",
            "But when it comes to satire, Colbert's got theicks!\n",
            "\n",
            "(Chorus - Stephen Colbert)\n",
            "So step up, John Oliver, and try to keep up with me,\n",
            "But when it comes to comedy, Colbert's got the key!\n",
            "\n",
            "(Verse 2 - John Oliver)\n",
            "Stephen Colbert, you may have the report,\n",
            "But when it comes to taking on the powerful, I've got the sort!\n",
            "\n",
            "(Chorus - John Oliver)\n",
            "So come on, Stephen Colbert, and try to keep up with me,\n",
            "But when it comes to holding the powerful accountable, Oliver's got the key!\n",
            "\n",
            "(Outro - Both)\n",
            "So there you have it, a rap battle for the ages,\n",
            "Stephen Colbert and John Oliver, we'll let you engage,\n",
            "But no matter who comes out on top, one thing's for sure, satire's alive and well, and Colbert and Oliver, they're just doing their best!\n",
            "\n",
            "(End of Rap Battle)"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    8770.28 ms\n",
            "llama_print_timings:      sample time =     184.92 ms /   282 runs   (    0.66 ms per token,  1525.01 tokens per second)\n",
            "llama_print_timings: prompt eval time =   15907.55 ms /    16 tokens (  994.22 ms per token,     1.01 tokens per second)\n",
            "llama_print_timings:        eval time =  256126.45 ms /   281 runs   (  911.48 ms per token,     1.10 tokens per second)\n",
            "llama_print_timings:       total time =  274147.36 ms /   297 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Answer: (Verse 1 - Stephen Colbert)\\nYo, I'm the Colbert Report,\\nI got the wit, I got the report,\\nNow John Oliver, you think you're so slick,\\nBut when it comes to satire, Colbert's got theicks!\\n\\n(Chorus - Stephen Colbert)\\nSo step up, John Oliver, and try to keep up with me,\\nBut when it comes to comedy, Colbert's got the key!\\n\\n(Verse 2 - John Oliver)\\nStephen Colbert, you may have the report,\\nBut when it comes to taking on the powerful, I've got the sort!\\n\\n(Chorus - John Oliver)\\nSo come on, Stephen Colbert, and try to keep up with me,\\nBut when it comes to holding the powerful accountable, Oliver's got the key!\\n\\n(Outro - Both)\\nSo there you have it, a rap battle for the ages,\\nStephen Colbert and John Oliver, we'll let you engage,\\nBut no matter who comes out on top, one thing's for sure, satire's alive and well, and Colbert and Oliver, they're just doing their best!\\n\\n(End of Rap Battle)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = prompt | llm"
      ],
      "metadata": {
        "id": "1Kp47CKzYfFX"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
        "llm_chain.invoke({\"question\": question})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "8CHtomllYK8o",
        "outputId": "11402ec3-f70f-4a47-d4b0-d594f6802559"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "First, let's find the year Justin Bieber was born. Justin Bieber was born on March 1, 1994.\n",
            "\n",
            "Next, let's find which NFL team won the Super Bowl in the year Justin Bieber was born.\n",
            "\n",
            "The Super Bowl was first played on January 15, 1967. Therefore, the Super Bowl wasn't played in the same year as Justin Bieber's birth.\n",
            "\n",
            "Therefore, there is no NFL team that won the Super Bowl in the year Justin Bieber was born because the Super Bowl wasn't played in that year."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =    8770.28 ms\n",
            "llama_print_timings:      sample time =      88.11 ms /   138 runs   (    0.64 ms per token,  1566.19 tokens per second)\n",
            "llama_print_timings: prompt eval time =   31100.06 ms /    45 tokens (  691.11 ms per token,     1.45 tokens per second)\n",
            "llama_print_timings:        eval time =  123643.38 ms /   137 runs   (  902.51 ms per token,     1.11 tokens per second)\n",
            "llama_print_timings:       total time =  155726.70 ms /   182 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nFirst, let's find the year Justin Bieber was born. Justin Bieber was born on March 1, 1994.\\n\\nNext, let's find which NFL team won the Super Bowl in the year Justin Bieber was born.\\n\\nThe Super Bowl was first played on January 15, 1967. Therefore, the Super Bowl wasn't played in the same year as Justin Bieber's birth.\\n\\nTherefore, there is no NFL team that won the Super Bowl in the year Justin Bieber was born because the Super Bowl wasn't played in that year.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}